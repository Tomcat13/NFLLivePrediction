{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ru1D3T5R3CK2"},"outputs":[],"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n","!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n","!pip install -q findspark"]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"spark-3.2.0-bin-hadoop3.2\"\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","sc = spark.sparkContext\n","sc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"-6t-eErR5zob","executionInfo":{"status":"ok","timestamp":1733710760194,"user_tz":300,"elapsed":14042,"user":{"displayName":"Aiden Tierney","userId":"16727660413695806448"}},"outputId":"9b05b0d4-4b0a-4818-8367-76b02c613700"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://b5e485910b8d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.2.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["pip install nfllivepy"],"metadata":{"id":"r3Pd9UGF51NX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# upload models\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZdJm0HX7D1r","executionInfo":{"status":"ok","timestamp":1733710789993,"user_tz":300,"elapsed":20893,"user":{"displayName":"Aiden Tierney","userId":"16727660413695806448"}},"outputId":"198d6dd2-aeab-4eeb-b270-ec1b20f88e64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\n","from pyspark.ml.regression import GeneralizedLinearRegressionModel\n","from google.colab import drive\n","from pyspark.sql.functions import col, when\n","\n","Q1model_path = \"/content/drive/MyDrive/Cloud NFL/Q1Pipeline\"\n","Q2model_path = \"/content/drive/MyDrive/Cloud NFL/Q2Pipeline\"\n","Q3model_path = \"/content/drive/MyDrive/Cloud NFL/Q3Pipeline\"\n","\n","Q1model = PipelineModel.load(Q1model_path)\n","Q2model = PipelineModel.load(Q2model_path)\n","Q3model = PipelineModel.load(Q3model_path)"],"metadata":{"id":"E9o7uesJ7LZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all functions needed ahead of time\n","\n","from nfllivepy.requester.pbp_requester import PBPRequester\n","import pandas as pd\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import lit\n","from pyspark.sql.functions import col, when, abs\n","import time"],"metadata":{"id":"uj8ngjpH55Vt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","\n","schema = StructType([\n","    StructField(\"posteam\", StringType(), True),\n","    StructField(\"home_score\", StringType(), True),\n","    StructField(\"away_score\", StringType(), True),\n","    StructField(\"quarter\", StringType(), True)\n","])"],"metadata":{"id":"lGfRBg1X5766"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of NFL teams with their home/away status\n","# must set whether the team is home (1) or away (0)\n","data = [\n","    (\"ATL\", 1),\n","    (\"CAR\", 1),\n","    (\"CLE\", 0),\n","    (\"JAX\", 1),\n","    (\"MIA\", 0),\n","    (\"MIN\", 0),\n","    (\"NO\", 1),\n","    (\"NYG\", 0),\n","    (\"NYJ\", 1),\n","    (\"OAK\", 1),\n","    (\"PHI\", 0),\n","    (\"PIT\", 1),\n","    (\"TEN\", 0),\n","    (\"SEA\", 0),\n","    (\"ARI\", 1),\n","    (\"CHI\", 0),\n","    (\"CIN\", 0),\n","    (\"DAL\", 1),\n","    (\"DEN\", 1),\n","    (\"DET\", 0),\n","    (\"GB\", 0),\n","    (\"HOU\", 1)\n","    #(\"FN\", 0)\n","    # Add more teams as needed\n","]\n","columns = [\"posteam\", \"is_home\"]\n","teams_df = spark.createDataFrame(data, columns)"],"metadata":{"id":"2_jyp3SV6xnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["requester = PBPRequester()"],"metadata":{"id":"rF2LHqju6ATf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_live_data():\n","  live_data = requester.get_live_pbp_all_games()\n","  game_data = live_data.loc[:, [\"posteam\", \"home_score\", \"away_score\", \"quarter\"]]\n","  games = spark.createDataFrame(game_data, schema=schema)\n","  games.createOrReplaceTempView(\"NFL\")\n","\n","  # data transformations\n","  # gets points\n","  quarters_home = spark.sql(\"\\\n","  SELECT `posteam`, `quarter`, MAX(home_score) as score \\\n","  FROM NFL \\\n","  GROUP BY `posteam`, `quarter` \\\n","  \")\n","  quarters_away = spark.sql(\"\\\n","  SELECT `posteam`, `quarter`, MAX(away_score) as score \\\n","  FROM NFL \\\n","  GROUP BY `posteam`, `quarter` \\\n","  \")\n","  # home pivot\n","  df_pivot = (\n","      quarters_home.groupBy('posteam')\n","      .pivot(\"quarter\", [1,2,3,4])  # Specify the unique values of 'time' if known\n","      .agg(F.first(\"score\"))  # Aggregate by taking the first score for each 'time'\n","  )\n","  # away pivot\n","  df_pivot_away = (\n","      quarters_away.groupBy('posteam')\n","      .pivot(\"quarter\", [1,2,3,4])  # Specify the unique values of 'time' if known\n","      .agg(F.first(\"score\"))  # Aggregate by taking the first score for each 'time'\n","  )\n","\n","  # create column \"Home\", home = 1, away = 0\n","\n","  df_pivot = df_pivot.withColumn(\"Home\", lit(1))\n","  df_pivot_away = df_pivot_away.withColumn(\"Home\", lit(0))\n","\n","  df_new = df_pivot.union(df_pivot_away)\n","\n","\n","  # making new feature columns\n","  # creates weights for the scores\n","  df_new = df_new.withColumn(\"weighted_1\", col(\"1\") * 0.5)\n","  df_new = df_new.withColumn(\"weighted_2\", col(\"2\") * 1.0)\n","  df_new = df_new.withColumn(\"weighted_3\", col(\"3\") * 1.5)\n","  # gets individual points scored in each quarter\n","  df_new = df_new.withColumn(\"q2pts\", col(\"2\") - col(\"1\"))\n","  df_new = df_new.withColumn(\"q3pts\", col(\"3\") - col(\"2\"))\n","  # creates interaction effects\n","  df_new = df_new.withColumn(\"interaction_1_2\", col(\"1\") * col(\"2\"))\n","  df_new = df_new.withColumn(\"interaction_2_3\", col(\"2\") * col(\"3\"))\n","  df_new = df_new.withColumn(\"interaction_1_3\", col(\"1\") * col(\"3\"))\n","\n","  df_new = df_new.join(teams_df, on=\"posteam\", how=\"left\")\n","\n","  # only keep instances where teams are correct\n","  df_new = df_new.filter(col(\"Home\") == col(\"is_home\"))\n","\n","  # to ensure the model works as desired\n","  df_new = df_new.withColumn(\"1\", col(\"1\").cast(IntegerType()))\n","  df_new = df_new.withColumn(\"2\", col(\"2\").cast(IntegerType()))\n","  df_new = df_new.withColumn(\"3\", col(\"3\").cast(IntegerType()))\n","\n","  pQ1 = PipelineModel.load(Q1model_path).transform(df_new)\n","  # excludes games that are in the first quarter\n","  Q2_data = df_new.filter(F.col(\"2\").isNotNull())\n","  pQ2 = PipelineModel.load(Q2model_path).transform(Q2_data)\n","  # excludes games that are in the second quarter\n","  Q3_data = df_new.filter(F.col(\"3\").isNotNull())\n","  pQ3 = PipelineModel.load(Q3model_path).transform(Q3_data)\n","\n","  pQ1 = pQ1.select(\"posteam\", \"prediction\").withColumnRenamed(\"prediction\", \"prediction_Q1\")\n","  pQ2 = pQ2.select(\"posteam\", \"prediction\").withColumnRenamed(\"prediction\", \"prediction_Q2\")\n","  pQ3 = pQ3.select(\"posteam\", \"prediction\").withColumnRenamed(\"prediction\", \"prediction_Q3\")\n","\n","  # prints final predictions\n","  predictions = pQ1.join(pQ2, on=\"posteam\", how=\"left\")\n","  predictions = predictions.join(pQ3, on=\"posteam\", how=\"left\")\n","  return predictions.show()"],"metadata":{"id":"CeDhuEkC6RAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","  get_live_data()\n","  time.sleep(10)"],"metadata":{"id":"N6vGKbtj_la7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"TdpTrnWG68Lx"}}]}